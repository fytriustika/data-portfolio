{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Scenario \n",
        "As a Data Engineer, I will dig up information from https://id.wikipedia.org/wiki/Demografi_Indonesia using the Python programming language. And i will create a Python script to save the \"Population by Province\" table from the web in CSV format.\n",
        "\n",
        "Web Scraping is one method that we can use to collect data from the internet. We do scraping if we want to extract information from a website directly using the http protocol.\n",
        "Web Scraping can be a solution in getting information from a website if the site does not provide an API for information retrieval. The following are the steps that can be taken in Web Scraping.\n",
        "The libraries we need are:\n",
        "1. Pandas, usually used to perform statistical data processing that is flexible, expressive and fast. This time I use it to extract data in the form of tables or data frames, so that the data can simplify the analysis process.\n",
        "2.Requests, used to send various HTTP requests and return a Response Object\n",
        "3.BeautifulSoup, serves as a parser to separate HTML components into a series of elements that are easy to read.\n",
        " \n"
      ],
      "metadata": {
        "id": "eKmGV9-Fkp7v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About This Dataset\n",
        "\n",
        "The columns to retrieve are as follows.\n",
        "\n",
        "* \n",
        "Area (km2)\n",
        "*   Population (2010)\n",
        "* Population (2020)\n",
        "*   Province name\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bLGBjMygmbz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scraping"
      ],
      "metadata": {
        "id": "PwbmKKxUq2aE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import library yang dibutuhkan\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#buatlah request ke website\n",
        "website_url = requests.get('https://id.wikipedia.org/wiki/Demografi_Indonesia').text\n",
        "soup = BeautifulSoup(website_url, 'lxml')\n",
        "\n",
        "#ambil table dengan class 'wikitable sortable'\n",
        "my_table = soup.find('table', {'class':'wikitable sortable'})\n",
        "\n",
        "#cari data dengan tag 'td'\n",
        "links = my_table.findAll('td')\n",
        "\n",
        "#buatlah lists kosong\n",
        "nama = []\n",
        "luas_km = []\n",
        "populasi10 = []\n",
        "populasi20 = []\n",
        "\n",
        "#memasukkan data ke dalam list berdasarkan pola HTML\n",
        "for i, link in enumerate(links):\n",
        "\tif i in range(0, len(links), 4):\n",
        "\t\tnama.append(link.get_text()[:-1])\n",
        "\tif i in range(1, len(links), 4):\n",
        "\t\tluas_km.append(link.get_text()[:-1])\n",
        "\tif i in range(2, len(links), 4):\n",
        "\t\tpopulasi10.append(link.get_text()[:-1])\n",
        "\tif i in range(3, len(links), 4):\n",
        "\t\tpopulasi20.append(link.get_text()[:-1])\n",
        "#buatlah DatFrame dan masukkan ke CSV\n",
        "df = pd.DataFrame()\n",
        "df['Nama Provinsi'] = nama\n",
        "df['Luas km'] = luas_km\n",
        "df['Populasi 2010'] = populasi10\n",
        "df['Populasi 2020'] = populasi20\n",
        "df.to_csv('Indonesia_Demography_by_Province.csv', index=False, encoding='utf-8', quoting=1)\n"
      ],
      "metadata": {
        "id": "_YvRU0plxAGR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWhs7qhNuLmU",
        "outputId": "c8e60167-3526-475f-ce02-6e56441e7054"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Nama Provinsi    Luas km Populasi 2010 Populasi 2020\n",
            "0           Aceh  56.500,51     4.494.410     5.274.871\n",
            "1  Sumatra Utara  72.427,81    12.982.204    14.799.361\n",
            "2  Sumatra Barat  42.224,65     4.846.909     5.534.472\n",
            "3           Riau  87.844,23     5.538.367     6.394.087\n",
            "4          Jambi  45.348,49     3.092.265     3.548.228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('Indonesia_Demography_by_Province.csv')"
      ],
      "metadata": {
        "id": "HDcfUa78xg7z"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function and Regular Expression\n",
        "\n",
        "As a Data Engineer, I was asked to create a function called \"email_check\" to filter several emails using the Python programming language. This function will accept a parameter named \"input\" which is an email and it will be either \"Pass\" or \"NotPass\".\n",
        "\n",
        "In this case I use the Regular Expression library alias re. A regular expression (regex) is a string of characters that defines a search pattern. These patterns are commonly used by string search algorithms to perform \"search\" or \"find and replace\" operations on strings, or to inspect input strings."
      ],
      "metadata": {
        "id": "i8IAZnKJxmce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import library yang dibutuhkan\n",
        "import re\n",
        "\n",
        "# buat function email_check\n",
        "def email_check(input):\n",
        "\tmatch = re.search('(?=^((?!-).)*$)(?=[^0-9])((?=^((?!\\.\\d).)*$)|(?=.*_))',input)\n",
        "\tif match:\n",
        "\t\tprint('Pass')\n",
        "\telse:\n",
        "\t\tprint('Not Pass')\n",
        "\n",
        "#Masukkan data email ke dalam list\n",
        "emails = ['my-name@someemail.com', 'myname@someemail.com', 'my.name@someemail.com', 'my.name2019@someemail.com', 'my.name.2019@someemail.com', 'somename.201903@someemail.com', 'my_name.201903@someemail.com', '201903myname@someemail.com', '201903.myname@someemail.com']\n",
        "\n",
        "#Looping untuk pengecekan Pass atau Not Pass, gunakan variabel email untuk meng-iterasi emails\n",
        "for email in emails :\n",
        "\temail_check(email)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-F8JerQcxolY",
        "outputId": "83bc5910-a3a3-410d-ef18-ed6f470f1a90"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not Pass\n",
            "Pass\n",
            "Pass\n",
            "Pass\n",
            "Not Pass\n",
            "Not Pass\n",
            "Pass\n",
            "Not Pass\n",
            "Not Pass\n"
          ]
        }
      ]
    }
  ]
}